{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29e8db9-fed9-4992-b291-7af9e1cca854",
   "metadata": {
    "id": "b29e8db9-fed9-4992-b291-7af9e1cca854"
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdbb647-d73f-468d-b876-8a81499804a1",
   "metadata": {
    "id": "7bdbb647-d73f-468d-b876-8a81499804a1"
   },
   "outputs": [],
   "source": [
    "# The 3 classes\n",
    "labels = ['COVID', 'NORMAL', 'PNEUMONIA']\n",
    "# The image size is 256*256\n",
    "img_size = 256\n",
    "# Converts the data list into a NumPy array with dtype=object,\n",
    "# containing pairs of resized image arrays and their respective class numbers (0,1,2) for (COVID, NORMAL, PNEUMONIA)\n",
    "def get_data(data_dir):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label).replace(\"\\\\\", \"/\")\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img).replace(\"\\\\\", \"/\")\n",
    "                img_arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c74d44d-b1cf-452e-9122-a6d51b894f0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c74d44d-b1cf-452e-9122-a6d51b894f0d",
    "outputId": "26e2e073-c22d-4e66-a9f2-4c44fd4e26e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n",
      "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "train = get_data('DataSet/Train')\n",
    "# Testing data\n",
    "test = get_data('DataSet/Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d6231-b55f-4663-8813-ea45606594e5",
   "metadata": {
    "id": "bb7d6231-b55f-4663-8813-ea45606594e5"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73208790-b0b7-4b3b-a93a-1100a4508152",
   "metadata": {
    "id": "73208790-b0b7-4b3b-a93a-1100a4508152"
   },
   "outputs": [],
   "source": [
    "# Contain the features and labels for the training set\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "# Contain the features and labels for the testing set\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature) # Represents the image data\n",
    "    y_train.append(label)  # Represents the class label associated with the image\n",
    "\n",
    "for feature, label in test:\n",
    "    x_test.append(feature) # Represents the image data\n",
    "    y_test.append(label) # Represents the class label associated with the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bfac4-48e7-459c-aa7a-a3d215fdeabc",
   "metadata": {
    "id": "cf3bfac4-48e7-459c-aa7a-a3d215fdeabc"
   },
   "source": [
    "# Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081da6f3-a313-46e2-a970-cb1a4186a757",
   "metadata": {
    "id": "081da6f3-a313-46e2-a970-cb1a4186a757"
   },
   "outputs": [],
   "source": [
    "# Converts the image pixel values from the range [0, 255] to a normalized range [0, 1] by dividing each pixel value by 255\n",
    "x_train = np.array(x_train) / 255\n",
    "x_test = np.array(x_test) / 255\n",
    "\n",
    "# resize data for deep learning\n",
    "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da968a62-fa0f-45ca-9e09-2a7c945730e2",
   "metadata": {
    "id": "da968a62-fa0f-45ca-9e09-2a7c945730e2"
   },
   "source": [
    "# Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb97506-49a7-4d22-8ca0-7ecd029d18ad",
   "metadata": {
    "id": "8fb97506-49a7-4d22-8ca0-7ecd029d18ad"
   },
   "outputs": [],
   "source": [
    "# Data augmentation is a technique used to artificially increase the diversity of the training dataset\n",
    "# by applying various transformations to the existing images.\n",
    "# This helps the model generalize better to new, unseen data and reduce overfitting.\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc9cd8-ef9f-4ca4-a349-23faf9de9f55",
   "metadata": {
    "id": "08cc9cd8-ef9f-4ca4-a349-23faf9de9f55"
   },
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4393d6ca-5c1a-4fe7-bd37-1369ad3b1cc5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4393d6ca-5c1a-4fe7-bd37-1369ad3b1cc5",
    "outputId": "5064548c-d75c-4a12-ccf1-f6067aebabc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 246016)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               31490176  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31509379 (120.20 MB)\n",
      "Trainable params: 31509379 (120.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd10d233-bc91-42fb-be25-d9c3b9030210",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd10d233-bc91-42fb-be25-d9c3b9030210",
    "outputId": "10940823-cf54-40b7-ee16-9e4fd010c27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "14/14 [==============================] - 40s 2s/step - loss: 3.8142 - accuracy: 0.3833 - val_loss: 1.0942 - val_accuracy: 0.3781\n",
      "Epoch 2/15\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.9625 - accuracy: 0.5020 - val_loss: 0.6003 - val_accuracy: 0.7401\n",
      "Epoch 3/15\n",
      "14/14 [==============================] - 21s 2s/step - loss: 0.6670 - accuracy: 0.7090 - val_loss: 0.3997 - val_accuracy: 0.8571\n",
      "Epoch 4/15\n",
      "14/14 [==============================] - 23s 2s/step - loss: 0.5304 - accuracy: 0.7876 - val_loss: 0.3382 - val_accuracy: 0.8876\n",
      "Epoch 5/15\n",
      "14/14 [==============================] - 21s 2s/step - loss: 0.4708 - accuracy: 0.8175 - val_loss: 0.3127 - val_accuracy: 0.8962\n",
      "Epoch 6/15\n",
      "14/14 [==============================] - 23s 2s/step - loss: 0.4298 - accuracy: 0.8400 - val_loss: 0.3125 - val_accuracy: 0.8944\n",
      "Epoch 7/15\n",
      "14/14 [==============================] - 21s 2s/step - loss: 0.4349 - accuracy: 0.8377 - val_loss: 0.3913 - val_accuracy: 0.8606\n",
      "Epoch 8/15\n",
      "14/14 [==============================] - 26s 2s/step - loss: 0.3858 - accuracy: 0.8528 - val_loss: 0.2927 - val_accuracy: 0.9002\n",
      "Epoch 9/15\n",
      "14/14 [==============================] - 21s 2s/step - loss: 0.3354 - accuracy: 0.8847 - val_loss: 0.2708 - val_accuracy: 0.9059\n",
      "Epoch 10/15\n",
      "14/14 [==============================] - 21s 1s/step - loss: 0.4080 - accuracy: 0.8394 - val_loss: 0.3351 - val_accuracy: 0.8841\n",
      "Epoch 11/15\n",
      "14/14 [==============================] - 23s 2s/step - loss: 0.3798 - accuracy: 0.8605 - val_loss: 0.3034 - val_accuracy: 0.8910\n",
      "Epoch 12/15\n",
      "14/14 [==============================] - 21s 2s/step - loss: 0.3421 - accuracy: 0.8744 - val_loss: 0.3430 - val_accuracy: 0.8944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b52f7b13520>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original labels were integers representing classes ( 0, 1, 2 for COVID, NORMAL, PNEUMONIA)\n",
    "# to_categorical converts these into arrays where each element corresponds to a class,\n",
    "# and the label is marked as 1 in the respective class position while other positions are 0s.\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "y_train_encoded = to_categorical(y_train,num_classes=3)\n",
    "y_test_encoded = to_categorical(y_test,num_classes=3)\n",
    "# Define an EarlyStopping callback to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "#  Training the neural network model using the prepared data\n",
    "model.fit(datagen.flow(x_train, y_train_encoded, batch_size=256), epochs=15, verbose=1, validation_data=(x_test, y_test_encoded),callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Train the model with modified parameters\n",
    "# model.fit(datagen.flow(x_train, y_train_encoded, batch_size=128),\n",
    "#           epochs=15,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test_encoded),\n",
    "#           callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf1c480-b096-4ff0-ba8b-bb006b2d76dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecf1c480-b096-4ff0-ba8b-bb006b2d76dc",
    "outputId": "798bf284-7a70-4754-d530-891459b62185"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 16ms/step - loss: 0.2708 - accuracy: 0.9059\n",
      "Loss of the model is - 0.2707638740539551\n",
      "Accuracy of the model is - 90.59093594551086 %\n"
     ]
    }
   ],
   "source": [
    "model.save('poumons_classifier.h5')\n",
    "# Assuming y_test is your original class labels (not one-hot encoded)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=3)  # Assuming you have 3 classes\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluation_result = model.evaluate(x_test, y_test_encoded)\n",
    "\n",
    "# Print the loss and accuracy\n",
    "print(\"Loss of the model is -\", evaluation_result[0])\n",
    "print(\"Accuracy of the model is -\", evaluation_result[1] * 100, \"%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9f9b0-8ec4-43c6-b163-40acb5e1f73a",
   "metadata": {
    "id": "fdf9f9b0-8ec4-43c6-b163-40acb5e1f73a"
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ca995-2f6a-40de-a6c7-8dff38521e21",
   "metadata": {
    "id": "aa7ca995-2f6a-40de-a6c7-8dff38521e21",
    "outputId": "76f9e5ac-013d-4448-d067-d92ec1a75087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 19s 345ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions = model.predict(x_test)\n",
    "predictions = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa124632-df97-4172-9772-bb080b8960cc",
   "metadata": {
    "id": "fa124632-df97-4172-9772-bb080b8960cc"
   },
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e7640b-efd8-4f29-8050-2e652a7dda10",
   "metadata": {
    "id": "d9e7640b-efd8-4f29-8050-2e652a7dda10"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    img = img.convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "    # Resize the image to the desired dimensions\n",
    "    target_size = (256, 256)  # Set the target size\n",
    "    img = img.resize(target_size, Image.ANTIALIAS)\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img)\n",
    "\n",
    "    # Normalize the pixel values to be in the range [0, 1]\n",
    "    img_array = img_array.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Add a batch dimension and reshape to match the model's input shape\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add a batch dimension\n",
    "\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685fb80a-3c83-4533-9e9a-6522a6ecff63",
   "metadata": {
    "id": "685fb80a-3c83-4533-9e9a-6522a6ecff63"
   },
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    from keras.models import load_model\n",
    "    # The output image will be a grayscale, resized, normalized, and reshaped representation of the original image\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "\n",
    "    model = load_model('poumons_classifier.h5')\n",
    "    # Make prediction on the single preprocessed image\n",
    "    prediction = model.predict(preprocessed_image)\n",
    "\n",
    "    # Get the index of the predicted class\n",
    "    predicted_class = prediction.argmax()\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0c9cb4-97cd-4a04-a094-7e97180338c2",
   "metadata": {
    "id": "3a0c9cb4-97cd-4a04-a094-7e97180338c2",
    "outputId": "46beef67-e1f1-44b5-a7fd-1a1fc92e1538"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'info.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Create the tooltip for the button\u001b[39;00m\n\u001b[0;32m    112\u001b[0m tooltip_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow accuracy and confusion matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 113\u001b[0m btn_show_info \u001b[38;5;241m=\u001b[39m \u001b[43mImageTk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPhotoImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minfo.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m btn_info \u001b[38;5;241m=\u001b[39m tk\u001b[38;5;241m.\u001b[39mButton(frame, bg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#CEDDD9\u001b[39m\u001b[38;5;124m'\u001b[39m, image\u001b[38;5;241m=\u001b[39mbtn_show_info, command\u001b[38;5;241m=\u001b[39mopen_new_window)\n\u001b[0;32m    115\u001b[0m btn_info\u001b[38;5;241m.\u001b[39mgrid(row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, column\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, sticky\u001b[38;5;241m=\u001b[39mtk\u001b[38;5;241m.\u001b[39mSE)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EnvDeep\\lib\\site-packages\\PIL\\ImageTk.py:101\u001b[0m, in \u001b[0;36mPhotoImage.__init__\u001b[1;34m(self, image, size, **kw)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Tk compatibility: file or data\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43m_get_image_from_kw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;66;03m# got an image instead of a mode\u001b[39;00m\n\u001b[0;32m    105\u001b[0m         mode \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mmode\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EnvDeep\\lib\\site-packages\\PIL\\ImageTk.py:58\u001b[0m, in \u001b[0;36m_get_image_from_kw\u001b[1;34m(kw)\u001b[0m\n\u001b[0;32m     56\u001b[0m     source \u001b[38;5;241m=\u001b[39m BytesIO(kw\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EnvDeep\\lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'info.png'"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import ImageTk, Image\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Lung Illness Detection\")\n",
    "\n",
    "# Get the screen width and height\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()\n",
    "\n",
    "# Calculate the frame width and height as 75% of the screen\n",
    "frame_width = int(screen_width * 0.75)\n",
    "frame_height = int(screen_height * 0.75)\n",
    "\n",
    "# Set the frame dimensions\n",
    "root.geometry(f\"{frame_width}x{frame_height}\")\n",
    "\n",
    "frame = tk.Frame(root, bg='#CEDDD9', width=frame_width, height=frame_height)\n",
    "frame.pack_propagate(False)  # Disable frame auto-resizing\n",
    "frame.grid_propagate(False)  # Disable grid resizing\n",
    "frame.grid_rowconfigure(0, weight=1)  # Make row expandable\n",
    "frame.grid_columnconfigure(0, weight=1)  # Make column expandable\n",
    "\n",
    "lbl_show_pic = tk.Label(frame, bg='#CEDDD9') #Label of the selected image\n",
    "lbl_show_predictedClass = tk.Label(frame, bg='#CEDDD9', fg='#297C65', font=('verdana', 14, 'bold'))\n",
    "entry_show_predictedClass = tk.Entry(frame, font=('verdana',16)) #Value of the predicted class\n",
    "btn_browse = tk.Button(frame, text='Select Image', bg='grey', fg='#ffffff', font=('verdana', 16)) #Button select image\n",
    "btn_predict = tk.Button(frame, text='Predict', bg='#50AD94', fg='#ffffff', font=('verdana', 16)) #Button predict\n",
    "\n",
    "\n",
    "# The window that contains Accuracy and confusion matrix\n",
    "def open_new_window():\n",
    "    new_window = tk.Toplevel(root)\n",
    "    new_window.title(\"Informations\")\n",
    "\n",
    "    # Get the dimensions of the root frame\n",
    "    root_frame_width = frame.winfo_width()\n",
    "    root_frame_height = frame.winfo_height()\n",
    "\n",
    "    new_window.geometry(f\"{root_frame_width}x{root_frame_height}\")\n",
    "\n",
    "    frame1 = tk.Frame(new_window, bg='#CEDDD9', width=root_frame_width, height=root_frame_height)\n",
    "    frame1.pack_propagate(False)  # Disable frame auto-resizing\n",
    "    frame1.grid_propagate(False)  # Disable grid resizing\n",
    "    frame1.grid_rowconfigure(0, weight=1) # Make row expandable\n",
    "    frame1.grid_columnconfigure(0, weight=1)  # Make column expandable\n",
    "\n",
    "    # Label of accuracy\n",
    "    lbl_accuracy = tk.Label(frame1,text=\"Accuracy of the model is : \", bg='#CEDDD9', fg='#297C65', font=('verdana', 14, 'bold'))\n",
    "    lbl_value_accuracy = tk.Label(frame1,text=f\"{evaluation_result[1] * 100:.2f}%\", bg='#CEDDD9', fg='#000000', font=('verdana', 12, 'bold'))\n",
    "    lbl_accuracy.pack()\n",
    "    lbl_value_accuracy.pack()\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    labels = ['0', '1', '2']  # Update labels based on your classes\n",
    "\n",
    "    # Create a DataFrame for the confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    # Create a heatmap using Seaborn\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm_df, cmap=\"Blues\", linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "    # Convert the plot to an image\n",
    "    plt.savefig('confusion_matrix.png')  # Save the plot as an image\n",
    "\n",
    "    # Display the confusion matrix image in the new window\n",
    "    img1 = Image.open('confusion_matrix.png')\n",
    "    img1 = img1.resize((512, 512), Image.ANTIALIAS)\n",
    "    img1 = ImageTk.PhotoImage(img1)\n",
    "\n",
    "    # Label of confusion matrix\n",
    "    lbl_matrice_confusion = tk.Label(frame1, text=\"Confusion Matrix : \", bg='#CEDDD9', fg='#297C65', font=('verdana', 14, 'bold'))\n",
    "    lbl_matrice_confusion.pack()\n",
    "    lbl_show_matrice_confusion = tk.Label(frame1, bg='#CEDDD9', image=img1)\n",
    "    lbl_show_matrice_confusion.image = img1  # Store a reference to prevent image from being garbage collected\n",
    "    lbl_show_matrice_confusion.pack()\n",
    "\n",
    "    frame1.pack()\n",
    "\n",
    "# On Hover button Informations\n",
    "class ToolTip:\n",
    "    def __init__(self, widget, text):\n",
    "        self.widget = widget\n",
    "        self.text = text\n",
    "        self.tooltip_window = None\n",
    "        self.widget.bind(\"<Enter>\", self.show_tooltip)\n",
    "        self.widget.bind(\"<Leave>\", self.hide_tooltip)\n",
    "\n",
    "    def show_tooltip(self, event=None):\n",
    "        x, y, _, _ = self.widget.bbox(\"insert\")\n",
    "        x += self.widget.winfo_rootx() + 25\n",
    "        y += self.widget.winfo_rooty() + 25\n",
    "\n",
    "        self.tooltip_window = tk.Toplevel(self.widget)\n",
    "        self.tooltip_window.wm_overrideredirect(True)\n",
    "        self.tooltip_window.wm_geometry(f\"+{x}+{y}\")\n",
    "\n",
    "        label = tk.Label(self.tooltip_window, text=self.text, bg=\"#ffffff\", relief=\"solid\", borderwidth=1)\n",
    "        label.pack()\n",
    "\n",
    "    def hide_tooltip(self, event=None):\n",
    "        if self.tooltip_window:\n",
    "            self.tooltip_window.destroy()\n",
    "            self.tooltip_window = None\n",
    "\n",
    "# Create the tooltip for the button\n",
    "tooltip_text = \"Show accuracy and confusion matrix\"\n",
    "btn_show_info = ImageTk.PhotoImage(file='info.png')\n",
    "btn_info = tk.Button(frame, bg='#CEDDD9', image=btn_show_info, command=open_new_window)\n",
    "btn_info.grid(row=4, column=0, sticky=tk.SE)\n",
    "tooltip = ToolTip(btn_info, tooltip_text)\n",
    "\n",
    "# Set padding to create space from the bottom and right edges\n",
    "frame.grid_rowconfigure(5, minsize=20)  # Additional row for padding\n",
    "frame.grid_columnconfigure(1, minsize=20)  # Additional column for padding\n",
    "\n",
    "# Will contain the selected image path\n",
    "filename=\"\"\n",
    "\n",
    "# Function called when we select an image\n",
    "def selectPic():\n",
    "    global img, filename\n",
    "    filename = filedialog.askopenfilename(initialdir=\"/images\", title=\"Select Image\",\n",
    "                                          filetypes=((\"png images\", \"*.png\"), (\"jpg images\", \"*.jpg\")))\n",
    "    # Display the image\n",
    "    img = Image.open(filename)\n",
    "    img = img.resize((256, 256), Image.ANTIALIAS)\n",
    "    img = ImageTk.PhotoImage(img)\n",
    "    lbl_show_pic['image'] = img\n",
    "    lbl_show_pic.grid(row=0, column=0, padx=20, pady=20, sticky=tk.W + tk.E + tk.N + tk.S)\n",
    "    # Display a new button to predict the image's class\n",
    "    btn_predict.grid(row=2, column=0, padx=10, pady=10, sticky=tk.N)\n",
    "\n",
    "# Function called when we press the predict button\n",
    "def predictPic():\n",
    "    # Display predicted class\n",
    "     lbl_show_predictedClass.grid(row=3, column=0, padx=20, pady=20, sticky=tk.W + tk.E + tk.N + tk.S)\n",
    "     predicted_class = process_image(filename)\n",
    "     lbl_show_predictedClass.config(text=\"Predicted class : \"+labels[predicted_class])\n",
    "\n",
    "\n",
    "btn_browse['command'] = selectPic # Select image button\n",
    "btn_predict['command'] = predictPic # Predict class button\n",
    "\n",
    "# Display the select image button\n",
    "btn_browse.grid(row=1, column=0, padx=10, pady=10, sticky=tk.N)\n",
    "\n",
    "frame.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db21708-ea2b-4c9a-bcd8-bf1a34416d17",
   "metadata": {
    "id": "3db21708-ea2b-4c9a-bcd8-bf1a34416d17",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
